{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f279b329-1c7f-4a2c-905d-7d616fa36a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34541d67-3166-42d9-86ee-17dd47c5f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activations separately for indexing and instantiation\n",
    "activation_names = ['sigmoid', 'tanh', 'relu', 'elu', 'selu', 'swish']\n",
    "activation_map = {\n",
    "    'sigmoid': nn.Sigmoid,\n",
    "    'tanh': nn.Tanh,\n",
    "    'relu': nn.ReLU,\n",
    "    'elu': nn.ELU,\n",
    "    'selu': nn.SELU,\n",
    "    'swish': lambda: nn.SiLU(),  # swish ≈ SiLU in PyTorch\n",
    "}\n",
    "\n",
    "def decode_action(action_id):\n",
    "    if 0 <= action_id <= 41:  # Dense Layer with some number of units\n",
    "        units_list = [8, 16, 32, 64, 128, 256, 512]\n",
    "        units_idx = action_id // len(activation_names)\n",
    "        act_idx = action_id % len(activation_names)\n",
    "        units = units_list[units_idx]\n",
    "        activation_name = activation_names[act_idx]\n",
    "        activation_fn = activation_map[activation_name]\n",
    "        return ('dense', units, activation_name)  \n",
    "\n",
    "    elif 42 <= action_id <= 44:  # Dropout layer\n",
    "        dropout_rates = [0.0, 0.2, 0.5]\n",
    "        return ('dropout', dropout_rates[action_id - 42])\n",
    "\n",
    "    elif action_id == 45:  # BatchNorm layer\n",
    "        return ('batchnorm',)\n",
    "\n",
    "    elif action_id == 46:  # Stop building layers\n",
    "        return ('stop',)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid action id: {action_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41c5476e-3a80-4f67-9ad1-e2a220063844",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NASMLPEnv(Env):\n",
    "    def __init__(self, dataset, max_layers=6):\n",
    "        self.dataset = dataset  \n",
    "        self.max_layers = max_layers\n",
    "        self.action_space = Discrete(47)\n",
    "        self.observation_space = Box(low=0, high=1, shape=(max_layers * 3,), dtype=np.float32) # 3 inputs per layer, layer type, number of units, and activation function\n",
    "        self.architecture = []\n",
    "        self.done = False\n",
    "\n",
    "        self.best_architecture = None\n",
    "        self.best_reward = -float('inf')\n",
    "\n",
    "        self.architecture_log = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.architecture = []\n",
    "        self.done = False\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action_id):\n",
    "        decoded = decode_action(action_id)\n",
    "\n",
    "        if decoded[0] == 'stop' or len(self.architecture) >= self.max_layers:\n",
    "            self.done = True\n",
    "            reward = self._evaluate_model()\n",
    "            self.architecture_log.append({ # logging the results so that we can analyze later\n",
    "                'architecture': list(self.architecture),\n",
    "                'reward': reward\n",
    "            })\n",
    "            print(f\"\\n🎯 Final Architecture: {self.architecture}\")\n",
    "            print(f\"🏆 Validation Accuracy (Reward): {reward:.2f}%\\n\")\n",
    "\n",
    "            # Track best\n",
    "            if reward > self.best_reward:\n",
    "                self.best_reward = reward\n",
    "                self.best_architecture = list(self.architecture)\n",
    "                print(f\"🌟 New Best Architecture Found with Reward: {reward:.2f}%\")\n",
    "        else:\n",
    "            self.architecture.append(decoded)\n",
    "            reward = 0\n",
    "\n",
    "        return self._get_obs(), reward, self.done, {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs = np.zeros(self.observation_space.shape[0])\n",
    "        for i, layer in enumerate(self.architecture):\n",
    "            base = i * 3\n",
    "            if layer[0] == 'dense':\n",
    "                obs[base] = layer[1] / 512  # normalized units\n",
    "                obs[base + 1] = activation_names.index(layer[2]) / (len(activation_names) - 1)\n",
    "                obs[base + 2] = 0\n",
    "            elif layer[0] == 'dropout':\n",
    "                obs[base] = layer[1]\n",
    "                obs[base + 1] = -1\n",
    "                obs[base + 2] = 1\n",
    "            elif layer[0] == 'batchnorm':\n",
    "                obs[base] = -1\n",
    "                obs[base + 1] = -1\n",
    "                obs[base + 2] = 2\n",
    "        return obs\n",
    "\n",
    "    def _evaluate_model(self):\n",
    "        X_train, y_train, X_val, y_val = self.dataset\n",
    "\n",
    "        model = nn.Sequential()\n",
    "        input_dim = X_train.shape[1]\n",
    "\n",
    "        for i, layer in enumerate(self.architecture):\n",
    "            if layer[0] == 'dense':\n",
    "                model.add_module(f\"fc{i}\", nn.Linear(input_dim, layer[1]))\n",
    "                model.add_module(f\"act{i}\", activation_map[layer[2]]())\n",
    "                input_dim = layer[1]\n",
    "            elif layer[0] == 'dropout':\n",
    "                model.add_module(f\"dropout{i}\", nn.Dropout(p=layer[1]))\n",
    "            elif layer[0] == 'batchnorm':\n",
    "                model.add_module(f\"bn{i}\", nn.BatchNorm1d(input_dim))\n",
    "\n",
    "        model.add_module(\"output\", nn.Linear(input_dim, y_train.shape[1]))\n",
    "\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01) # can make the RL choose optimizer and LR also here\n",
    "        train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "\n",
    "        model.train()\n",
    "        for _ in range(3):  # Only 3 epochs, can change this\n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(xb)\n",
    "                loss = loss_fn(output, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = model(X_val)\n",
    "            acc = (preds.argmax(dim=1) == y_val.argmax(dim=1)).float().mean().item()\n",
    "\n",
    "        return acc * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b3cc697-1593-4053-88e2-3c3866116f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "import openml\n",
    "dataset = openml.datasets.get_dataset(42769)\n",
    "X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\n",
    "\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "X = df.drop(columns=[dataset.default_target_attribute])\n",
    "y = df[dataset.default_target_attribute]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)  \n",
    "\n",
    "y_oh = one_hot(torch.tensor(y_encoded)).float()\n",
    "\n",
    "X_train_np, X_val_np, y_train_tensor, y_val_tensor = train_test_split(\n",
    "    X_scaled, y_oh, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_np).float()\n",
    "X_val_tensor = torch.tensor(X_val_np).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4ee5701-7c65-4eef-850c-8f00aef3b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d3cfaf9-f53b-442b-978e-66d9b83a46c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([900000, 28])\n",
      "torch.Size([900000, 2])\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tensor.shape)  # [n_train, n_features]\n",
    "print(y_train_tensor.shape)  # [n_train, 2]  (one-hot binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "432cf817-7634-48d6-8ea0-a6375df2bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass into your NAS environment\n",
    "env = NASMLPEnv(dataset=dataset, max_layers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3231d0b6-0c15-4df7-9794-72c127704cfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./nas_logs/DQN_10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Final Architecture: [('dense', 16, 'swish'), ('dense', 128, 'sigmoid'), ('dense', 512, 'elu'), ('dense', 512, 'sigmoid'), ('dense', 16, 'selu')]\n",
      "🏆 Validation Accuracy (Reward): 52.95%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 16, 'tanh'), ('dense', 128, 'selu'), ('dense', 256, 'swish'), ('dense', 8, 'relu'), ('dense', 256, 'elu')]\n",
      "🏆 Validation Accuracy (Reward): 52.95%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 256, 'selu'), ('dense', 512, 'swish'), ('dense', 32, 'swish'), ('dense', 128, 'tanh'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 52.95%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 512, 'elu'), ('dense', 128, 'selu'), ('dense', 128, 'swish'), ('dense', 256, 'selu'), ('dense', 16, 'sigmoid')]\n",
      "🏆 Validation Accuracy (Reward): 52.95%\n",
      "\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6        |\n",
      "|    ep_rew_mean      | 53       |\n",
      "|    exploration_rate | 0.772    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 0        |\n",
      "|    time_elapsed     | 130      |\n",
      "|    total_timesteps  | 24       |\n",
      "----------------------------------\n",
      "\n",
      "🎯 Final Architecture: [('dense', 128, 'elu'), ('dense', 256, 'relu'), ('dense', 16, 'relu'), ('dense', 512, 'sigmoid'), ('dense', 128, 'swish')]\n",
      "🏆 Validation Accuracy (Reward): 52.95%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 128, 'elu'), ('dense', 256, 'elu'), ('dense', 16, 'relu'), ('dense', 8, 'elu'), ('dense', 8, 'elu')]\n",
      "🏆 Validation Accuracy (Reward): 70.78%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 128, 'sigmoid'), ('dense', 64, 'relu'), ('dense', 8, 'elu'), ('dense', 128, 'elu'), ('batchnorm',)]\n",
      "🏆 Validation Accuracy (Reward): 74.23%\n",
      "\n",
      "🌟 New Best Architecture Found with Reward: 74.23%\n",
      "\n",
      "🎯 Final Architecture: [('dense', 16, 'relu'), ('dense', 128, 'swish'), ('dense', 64, 'relu'), ('dense', 8, 'selu'), ('dense', 512, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 69.12%\n",
      "\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6        |\n",
      "|    ep_rew_mean      | 59.9     |\n",
      "|    exploration_rate | 0.544    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 0        |\n",
      "|    time_elapsed     | 226      |\n",
      "|    total_timesteps  | 48       |\n",
      "----------------------------------\n",
      "\n",
      "🎯 Final Architecture: [('dense', 32, 'selu'), ('dense', 8, 'selu'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 72.01%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('batchnorm',), ('dense', 128, 'sigmoid'), ('dense', 512, 'sigmoid'), ('dropout', 0.2), ('dense', 64, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 73.06%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 512, 'relu'), ('dense', 256, 'tanh'), ('dense', 32, 'relu'), ('dense', 16, 'selu'), ('dense', 16, 'elu')]\n",
      "🏆 Validation Accuracy (Reward): 69.73%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 64, 'tanh'), ('dense', 512, 'relu'), ('dense', 8, 'selu'), ('dense', 64, 'selu'), ('dropout', 0.0)]\n",
      "🏆 Validation Accuracy (Reward): 73.01%\n",
      "\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.83     |\n",
      "|    ep_rew_mean      | 63.9     |\n",
      "|    exploration_rate | 0.335    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 0        |\n",
      "|    time_elapsed     | 324      |\n",
      "|    total_timesteps  | 70       |\n",
      "----------------------------------\n",
      "\n",
      "🎯 Final Architecture: [('dense', 64, 'selu'), ('dense', 32, 'swish'), ('dense', 256, 'tanh'), ('dense', 8, 'tanh'), ('dense', 128, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 60.02%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 64, 'sigmoid'), ('dense', 8, 'elu'), ('dense', 32, 'tanh'), ('dense', 8, 'sigmoid'), ('dense', 128, 'swish')]\n",
      "🏆 Validation Accuracy (Reward): 72.71%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 8, 'elu'), ('dense', 16, 'elu'), ('dense', 512, 'sigmoid'), ('dense', 8, 'sigmoid'), ('dense', 32, 'tanh')]\n",
      "🏆 Validation Accuracy (Reward): 64.56%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 64, 'relu'), ('dense', 8, 'relu'), ('dense', 8, 'elu'), ('dense', 512, 'swish'), ('dense', 32, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 52.95%\n",
      "\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.88     |\n",
      "|    ep_rew_mean      | 63.6     |\n",
      "|    exploration_rate | 0.107    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 0        |\n",
      "|    time_elapsed     | 415      |\n",
      "|    total_timesteps  | 94       |\n",
      "----------------------------------\n",
      "\n",
      "🎯 Final Architecture: [('dense', 512, 'swish'), ('dense', 16, 'sigmoid'), ('dense', 16, 'tanh'), ('dense', 32, 'tanh'), ('dense', 64, 'sigmoid')]\n",
      "🏆 Validation Accuracy (Reward): 68.57%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 512, 'relu'), ('dense', 128, 'selu'), ('dense', 256, 'swish'), ('dense', 256, 'relu'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 54.45%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 512, 'relu'), ('dense', 128, 'selu'), ('dense', 256, 'swish'), ('dense', 128, 'tanh'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 52.95%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 512, 'relu'), ('dense', 128, 'selu'), ('dense', 256, 'swish'), ('dense', 256, 'relu'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 52.95%\n",
      "\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.9      |\n",
      "|    ep_rew_mean      | 62.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 0        |\n",
      "|    time_elapsed     | 562      |\n",
      "|    total_timesteps  | 118      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 18.2     |\n",
      "|    n_updates        | 4        |\n",
      "----------------------------------\n",
      "\n",
      "🎯 Final Architecture: [('dense', 512, 'relu'), ('dense', 128, 'selu'), ('dense', 256, 'swish'), ('dense', 256, 'relu'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 52.95%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 512, 'relu'), ('dense', 128, 'selu'), ('dense', 256, 'swish'), ('dense', 256, 'relu'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 52.95%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 512, 'relu'), ('dense', 128, 'selu'), ('dense', 256, 'swish'), ('dense', 256, 'relu'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 52.95%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 512, 'relu'), ('dense', 128, 'swish'), ('dense', 256, 'swish'), ('dense', 64, 'tanh'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 52.95%\n",
      "\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.92     |\n",
      "|    ep_rew_mean      | 60.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 0        |\n",
      "|    time_elapsed     | 745      |\n",
      "|    total_timesteps  | 142      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.13     |\n",
      "|    n_updates        | 10       |\n",
      "----------------------------------\n",
      "\n",
      "🎯 Final Architecture: [('dense', 512, 'relu'), ('dense', 128, 'selu'), ('dense', 256, 'swish'), ('dense', 256, 'relu'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 52.95%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 512, 'relu'), ('dense', 128, 'selu'), ('dense', 256, 'swish'), ('dense', 256, 'sigmoid'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 52.95%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 512, 'relu'), ('dense', 128, 'selu'), ('dense', 256, 'swish'), ('dense', 256, 'relu'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 52.95%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 512, 'relu'), ('dense', 128, 'selu'), ('dense', 256, 'swish'), ('dense', 256, 'relu'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 55.13%\n",
      "\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.93     |\n",
      "|    ep_rew_mean      | 59.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 0        |\n",
      "|    time_elapsed     | 933      |\n",
      "|    total_timesteps  | 166      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.65     |\n",
      "|    n_updates        | 16       |\n",
      "----------------------------------\n",
      "\n",
      "🎯 Final Architecture: [('dense', 512, 'relu'), ('dense', 128, 'selu'), ('dense', 256, 'swish'), ('dense', 256, 'relu'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 52.95%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 512, 'relu'), ('dense', 128, 'selu'), ('dense', 256, 'swish'), ('dense', 8, 'swish'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 52.95%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 512, 'relu'), ('dense', 128, 'selu'), ('dense', 256, 'swish'), ('dense', 256, 'relu'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 59.71%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 512, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 72.77%\n",
      "\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.94     |\n",
      "|    ep_rew_mean      | 59.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 0        |\n",
      "|    time_elapsed     | 1125     |\n",
      "|    total_timesteps  | 190      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 13.2     |\n",
      "|    n_updates        | 22       |\n",
      "----------------------------------\n",
      "\n",
      "🎯 Final Architecture: [('dense', 512, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 72.55%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 128, 'relu'), ('dense', 512, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 71.82%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 256, 'relu'), ('dense', 512, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 72.97%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 32, 'swish'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('batchnorm',), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 74.10%\n",
      "\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.94     |\n",
      "|    ep_rew_mean      | 61.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 0        |\n",
      "|    time_elapsed     | 1313     |\n",
      "|    total_timesteps  | 214      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 10.4     |\n",
      "|    n_updates        | 28       |\n",
      "----------------------------------\n",
      "\n",
      "🎯 Final Architecture: [('dense', 256, 'relu'), ('dense', 512, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 73.06%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 71.86%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 73.25%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 73.14%\n",
      "\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.95     |\n",
      "|    ep_rew_mean      | 62.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 0        |\n",
      "|    time_elapsed     | 1489     |\n",
      "|    total_timesteps  | 238      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 17.3     |\n",
      "|    n_updates        | 34       |\n",
      "----------------------------------\n",
      "\n",
      "🎯 Final Architecture: [('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 72.89%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 72.35%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 72.99%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 72.54%\n",
      "\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.95     |\n",
      "|    ep_rew_mean      | 63.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 0        |\n",
      "|    time_elapsed     | 1722     |\n",
      "|    total_timesteps  | 262      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6        |\n",
      "|    n_updates        | 40       |\n",
      "----------------------------------\n",
      "\n",
      "🎯 Final Architecture: [('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 32, 'selu'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 71.49%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 72.58%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 73.45%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu'), ('dense', 256, 'relu')]\n",
      "🏆 Validation Accuracy (Reward): 73.48%\n",
      "\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.96     |\n",
      "|    ep_rew_mean      | 64.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 0        |\n",
      "|    time_elapsed     | 2158     |\n",
      "|    total_timesteps  | 286      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.28     |\n",
      "|    n_updates        | 46       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      7\u001b[0m model_1000 \u001b[38;5;241m=\u001b[39m DQN(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./nas_logs/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m model_1000\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m     10\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     11\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/dqn/dqn.py:267\u001b[0m, in \u001b[0;36mDQN.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDQN,\n\u001b[1;32m    260\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDQN:\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mlearn(\n\u001b[1;32m    268\u001b[0m         total_timesteps\u001b[38;5;241m=\u001b[39mtotal_timesteps,\n\u001b[1;32m    269\u001b[0m         callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    270\u001b[0m         log_interval\u001b[38;5;241m=\u001b[39mlog_interval,\n\u001b[1;32m    271\u001b[0m         tb_log_name\u001b[38;5;241m=\u001b[39mtb_log_name,\n\u001b[1;32m    272\u001b[0m         reset_num_timesteps\u001b[38;5;241m=\u001b[39mreset_num_timesteps,\n\u001b[1;32m    273\u001b[0m         progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[1;32m    274\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/off_policy_algorithm.py:328\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 328\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect_rollouts(\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv,\n\u001b[1;32m    330\u001b[0m         train_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_freq,\n\u001b[1;32m    331\u001b[0m         action_noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_noise,\n\u001b[1;32m    332\u001b[0m         callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    333\u001b[0m         learning_starts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_starts,\n\u001b[1;32m    334\u001b[0m         replay_buffer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer,\n\u001b[1;32m    335\u001b[0m         log_interval\u001b[38;5;241m=\u001b[39mlog_interval,\n\u001b[1;32m    336\u001b[0m     )\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/off_policy_algorithm.py:560\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    557\u001b[0m actions, buffer_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    563\u001b[0m num_collected_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:222\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_wait()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:59\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 59\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[env_idx]\u001b[38;5;241m.\u001b[39mstep(  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m     60\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[env_idx]\n\u001b[1;32m     61\u001b[0m         )\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/shimmy/openai_gym_compatibility.py:250\u001b[0m, in \u001b[0;36mGymV21CompatibilityV0.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Any, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m    242\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m        (observation, reward, terminated, truncated, info)\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgym_env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m, in \u001b[0;36mNASMLPEnv.step\u001b[0;34m(self, action_id)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoded[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marchitecture) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_layers:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate_model()\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marchitecture_log\u001b[38;5;241m.\u001b[39mappend({ \u001b[38;5;66;03m# logging the results so that we can analyze later\u001b[39;00m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marchitecture\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marchitecture),\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m: reward\n\u001b[1;32m     29\u001b[0m     })\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🎯 Final Architecture: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marchitecture\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 91\u001b[0m, in \u001b[0;36mNASMLPEnv._evaluate_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_fn(output, yb)\n\u001b[1;32m     90\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 91\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     93\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/adam.py:244\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    232\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    234\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    235\u001b[0m         group,\n\u001b[1;32m    236\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m         state_steps,\n\u001b[1;32m    242\u001b[0m     )\n\u001b[0;32m--> 244\u001b[0m     adam(\n\u001b[1;32m    245\u001b[0m         params_with_grad,\n\u001b[1;32m    246\u001b[0m         grads,\n\u001b[1;32m    247\u001b[0m         exp_avgs,\n\u001b[1;32m    248\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    249\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    250\u001b[0m         state_steps,\n\u001b[1;32m    251\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    252\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    253\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    254\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    255\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    256\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    257\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    258\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    259\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    260\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    261\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    262\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    263\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    264\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    265\u001b[0m     )\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/adam.py:876\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    874\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 876\u001b[0m func(\n\u001b[1;32m    877\u001b[0m     params,\n\u001b[1;32m    878\u001b[0m     grads,\n\u001b[1;32m    879\u001b[0m     exp_avgs,\n\u001b[1;32m    880\u001b[0m     exp_avg_sqs,\n\u001b[1;32m    881\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m    882\u001b[0m     state_steps,\n\u001b[1;32m    883\u001b[0m     amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    884\u001b[0m     has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    885\u001b[0m     beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    886\u001b[0m     beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    887\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    888\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    889\u001b[0m     eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    890\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    891\u001b[0m     capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    892\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    893\u001b[0m     grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    894\u001b[0m     found_inf\u001b[38;5;241m=\u001b[39mfound_inf,\n\u001b[1;32m    895\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/adam.py:476\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    474\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 476\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    478\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model_1000 = DQN(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./nas_logs/\")\n",
    "model_1000.learn(total_timesteps=1000)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "467a8c19-db00-4d4e-98bd-4c6705b299fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"nas_agent_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da8416d-5ab3-4c66-9ecf-e45a4a818702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cb72656-f70e-46e4-8fc2-7ef1cc442aac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'elapsed_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(elapsed_time)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'elapsed_time' is not defined"
     ]
    }
   ],
   "source": [
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44b6e584-058b-48bc-bbaa-30bb91533b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dense', 128, 'sigmoid'), ('dense', 64, 'relu'), ('dense', 8, 'elu'), ('dense', 128, 'elu'), ('batchnorm',)]\n"
     ]
    }
   ],
   "source": [
    "print(env.best_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d17baaa9-fff2-428d-b3cd-23de971a5d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best Validation Accuracy (Reward): 74.23%\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n Best Validation Accuracy (Reward): {env.best_reward:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21ee793d-c416-4e19-91c3-44040558bacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing the different combinations\n",
    "import pandas as pd\n",
    "\n",
    "flat_data = []\n",
    "for run in env.architecture_log:\n",
    "    has_dropout = any(layer[0] == 'dropout' for layer in run['architecture'])\n",
    "    has_batchnorm = any(layer[0] == 'batchnorm' for layer in run['architecture'])\n",
    "\n",
    "    for layer in run['architecture']:\n",
    "        if layer[0] == 'dense':\n",
    "            flat_data.append({\n",
    "                'units': layer[1],\n",
    "                'activation': layer[2],\n",
    "                'dropout': has_dropout,\n",
    "                'batchnorm': has_batchnorm,\n",
    "                'reward': run['reward']\n",
    "            })\n",
    "df = pd.DataFrame(flat_data)\n",
    "df.to_csv(\"architecture_analysis.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69090296-1831-46f2-a12e-a67ed8143551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>units</th>\n",
       "      <th>activation</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batchnorm</th>\n",
       "      <th>reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>selu</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>69.476002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128</td>\n",
       "      <td>selu</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>69.476002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>69.476002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>256</td>\n",
       "      <td>swish</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>47.049001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>swish</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>47.049001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2759</th>\n",
       "      <td>256</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>73.479003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2760</th>\n",
       "      <td>256</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>73.479003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2761</th>\n",
       "      <td>256</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>73.479003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2762</th>\n",
       "      <td>256</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>73.479003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>256</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>73.479003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2764 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      units activation  dropout  batchnorm     reward\n",
       "0         8       selu     True      False  69.476002\n",
       "1       128       selu     True      False  69.476002\n",
       "2       128       relu     True      False  69.476002\n",
       "3       256      swish    False      False  47.049001\n",
       "4        64      swish    False      False  47.049001\n",
       "...     ...        ...      ...        ...        ...\n",
       "2759    256       relu    False      False  73.479003\n",
       "2760    256       relu    False      False  73.479003\n",
       "2761    256       relu    False      False  73.479003\n",
       "2762    256       relu    False      False  73.479003\n",
       "2763    256       relu    False      False  73.479003\n",
       "\n",
       "[2764 rows x 5 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21928371-321f-480f-88a5-29450b8e24cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze effect of different units in each layer\n",
    "# analyze effect of different activation functions\n",
    "# analyze the effect of different layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
