{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e000567-d1a9-48b5-875a-927cd38081c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcba1e7-9daf-47a2-bbbb-b933be33ea80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activations separately for indexing and instantiation\n",
    "activation_names = ['sigmoid', 'tanh', 'relu', 'elu', 'selu', 'swish']\n",
    "activation_map = {\n",
    "    'sigmoid': nn.Sigmoid,\n",
    "    'tanh': nn.Tanh,\n",
    "    'relu': nn.ReLU,\n",
    "    'elu': nn.ELU,\n",
    "    'selu': nn.SELU,\n",
    "    'swish': lambda: nn.SiLU(),  # swish â‰ˆ SiLU in PyTorch\n",
    "}\n",
    "\n",
    "def decode_action(action_id):\n",
    "    if 0 <= action_id <= 41:  # Dense Layer with some number of units\n",
    "        units_list = [8, 16, 32, 64, 128, 256, 512]\n",
    "        units_idx = action_id // len(activation_names)\n",
    "        act_idx = action_id % len(activation_names)\n",
    "        units = units_list[units_idx]\n",
    "        activation_name = activation_names[act_idx]\n",
    "        activation_fn = activation_map[activation_name]\n",
    "        return ('dense', units, activation_name)  \n",
    "\n",
    "    elif 42 <= action_id <= 44:  # Dropout layer\n",
    "        dropout_rates = [0.0, 0.2, 0.5]\n",
    "        return ('dropout', dropout_rates[action_id - 42])\n",
    "\n",
    "    elif action_id == 45:  # BatchNorm layer\n",
    "        return ('batchnorm',)\n",
    "\n",
    "    elif action_id == 46:  # Stop building layers\n",
    "        return ('stop',)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid action id: {action_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6081f6a7-50e1-4f6c-867c-dc17dcbe1ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look into changing this next time, add data specific features and start finding new datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48a5e4a-dc7d-43de-9def-58742361ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NASMLPEnv(Env):\n",
    "    def __init__(self, dataset, max_layers=10):\n",
    "        self.dataset = dataset  \n",
    "        self.max_layers = max_layers\n",
    "        self.action_space = Discrete(47)\n",
    "        self.observation_space = Box(low=0, high=1, shape=(max_layers * 3,), dtype=np.float32) # 3 inputs per layer, layer type, number of units, and activation function\n",
    "        self.architecture = []\n",
    "        self.done = False\n",
    "\n",
    "        self.best_architecture = None\n",
    "        self.best_reward = -float('inf')\n",
    "\n",
    "        self.architecture_log = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.architecture = []\n",
    "        self.done = False\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action_id):\n",
    "        decoded = decode_action(action_id)\n",
    "\n",
    "        if decoded[0] == 'stop' or len(self.architecture) >= self.max_layers:\n",
    "            self.done = True\n",
    "            reward, acc, complexity = self._evaluate_model()  \n",
    "\n",
    "            # Log architecture, reward, accuracy, and complexity\n",
    "            self.architecture_log.append({\n",
    "                'architecture': list(self.architecture),\n",
    "                'reward': reward,\n",
    "                'accuracy': acc,\n",
    "                'complexity': complexity\n",
    "            })\n",
    "    \n",
    "            print(f\"\\nðŸŽ¯ Final Architecture: {self.architecture}\")\n",
    "            print(f\"ðŸ† Validation Accuracy (acc): {acc:.2f}%\")\n",
    "            print(f\"âš™ï¸  Complexity (number of layers): {complexity}\")\n",
    "            print(f\"ðŸ… Reward (acc - penalty): {reward:.2f}%\\n\")\n",
    "\n",
    "            # Track best\n",
    "            if reward > self.best_reward:\n",
    "                self.best_reward = reward\n",
    "                self.best_architecture = list(self.architecture)\n",
    "                self.best_accuracy = acc  \n",
    "                self.best_complexity = complexity \n",
    "                print(f\"ðŸŒŸ New Best Architecture Found with Reward: {reward:.2f}%\")\n",
    "        else:\n",
    "            self.architecture.append(decoded)\n",
    "            reward = 0\n",
    "\n",
    "        return self._get_obs(), reward, self.done, {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs = np.zeros(self.observation_space.shape[0])\n",
    "        for i, layer in enumerate(self.architecture):\n",
    "            base = i * 3\n",
    "            if layer[0] == 'dense':\n",
    "                obs[base] = layer[1] / 512  # normalized units\n",
    "                obs[base + 1] = activation_names.index(layer[2]) / (len(activation_names) - 1)\n",
    "                obs[base + 2] = 0\n",
    "            elif layer[0] == 'dropout':\n",
    "                obs[base] = layer[1]\n",
    "                obs[base + 1] = -1\n",
    "                obs[base + 2] = 1\n",
    "            elif layer[0] == 'batchnorm':\n",
    "                obs[base] = -1\n",
    "                obs[base + 1] = -1\n",
    "                obs[base + 2] = 2\n",
    "        return obs\n",
    "\n",
    "    def _evaluate_model(self):\n",
    "        X_train, y_train, X_val, y_val = self.dataset\n",
    "\n",
    "        model = nn.Sequential()\n",
    "        input_dim = X_train.shape[1]\n",
    "\n",
    "        for i, layer in enumerate(self.architecture):\n",
    "            if layer[0] == 'dense':\n",
    "                model.add_module(f\"fc{i}\", nn.Linear(input_dim, layer[1]))\n",
    "                model.add_module(f\"act{i}\", activation_map[layer[2]]())\n",
    "                input_dim = layer[1]\n",
    "            elif layer[0] == 'dropout':\n",
    "                model.add_module(f\"dropout{i}\", nn.Dropout(p=layer[1]))\n",
    "            elif layer[0] == 'batchnorm':\n",
    "                model.add_module(f\"bn{i}\", nn.BatchNorm1d(input_dim))\n",
    "\n",
    "        model.add_module(\"output\", nn.Linear(input_dim, y_train.shape[1]))\n",
    "\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01) # can make the RL choose optimizer and LR also here\n",
    "        train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "\n",
    "        model.train()\n",
    "        for _ in range(3):  # Only 3 epochs, can change this\n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(xb)\n",
    "                loss = loss_fn(output, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = model(X_val)\n",
    "            acc = (preds.argmax(dim=1) == y_val.argmax(dim=1)).float().mean().item()\n",
    "\n",
    "        num_layers = len(self.architecture)\n",
    "        lambda_penalty = 0.25  \n",
    "\n",
    "        reward = ((1 - lambda_penalty) * acc * 100) - (lambda_penalty * (num_layers*100/self.max_layers))\n",
    "\n",
    "        return reward, acc * 100, (num_layers*100/self.max_layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
