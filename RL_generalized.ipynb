{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e000567-d1a9-48b5-875a-927cd38081c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fcba1e7-9daf-47a2-bbbb-b933be33ea80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activations separately for indexing and instantiation\n",
    "activation_names = ['sigmoid', 'tanh', 'relu', 'elu', 'selu', 'swish']\n",
    "activation_map = {\n",
    "    'sigmoid': nn.Sigmoid,\n",
    "    'tanh': nn.Tanh,\n",
    "    'relu': nn.ReLU,\n",
    "    'elu': nn.ELU,\n",
    "    'selu': nn.SELU,\n",
    "    'swish': lambda: nn.SiLU(),  # swish ≈ SiLU in PyTorch\n",
    "}\n",
    "\n",
    "def decode_action(action_id):\n",
    "    if 0 <= action_id <= 41:  # Dense Layer with some number of units\n",
    "        units_list = [8, 16, 32, 64, 128, 256, 512]\n",
    "        units_idx = action_id // len(activation_names)\n",
    "        act_idx = action_id % len(activation_names)\n",
    "        units = units_list[units_idx]\n",
    "        activation_name = activation_names[act_idx]\n",
    "        activation_fn = activation_map[activation_name]\n",
    "        return ('dense', units, activation_name)  \n",
    "\n",
    "    elif 42 <= action_id <= 44:  # Dropout layer\n",
    "        dropout_rates = [0.0, 0.2, 0.5]\n",
    "        return ('dropout', dropout_rates[action_id - 42])\n",
    "\n",
    "    elif action_id == 45:  # BatchNorm layer\n",
    "        return ('batchnorm',)\n",
    "\n",
    "    elif action_id == 46:  # Stop building layers\n",
    "        return ('stop',)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid action id: {action_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6081f6a7-50e1-4f6c-867c-dc17dcbe1ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look into changing this next time, add data specific features and start finding new datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f48a5e4a-dc7d-43de-9def-58742361ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NASMLPEnv(Env):\n",
    "    def __init__(self, dataset, max_layers=10):\n",
    "        self.dataset = dataset  \n",
    "        self.max_layers = max_layers\n",
    "        self.dataset_features = self._compute_dataset_features(dataset) \n",
    "        self.action_space = Discrete(47)\n",
    "        num_dataset_features = 4  \n",
    "        max_possible_layers = max_layers + 10  \n",
    "        self.observation_space = Box(\n",
    "            low=0, high=1,\n",
    "            shape=(max_possible_layers * 3 + num_dataset_features,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.architecture = []\n",
    "        self.done = False\n",
    "\n",
    "        self.best_architecture = None\n",
    "        self.best_reward = -float('inf')\n",
    "\n",
    "        self.architecture_log = []\n",
    "\n",
    "        self.max_parameters = self._estimate_max_parameters()\n",
    "\n",
    "    def _compute_dataset_features(self, dataset):\n",
    "        X_train, y_train, X_val, y_val = dataset\n",
    "        n_rows, n_features = X_train.shape\n",
    "        feature_std = X_train.std(dim=0).mean().item()\n",
    "        class_balance = y_train.sum(dim=0) / y_train.shape[0]\n",
    "        class_balance = class_balance.max().item()  \n",
    "\n",
    "        return np.array([\n",
    "            n_rows / 1e5,            \n",
    "            n_features / 1e3,        \n",
    "            feature_std / 10.0,       \n",
    "            class_balance             \n",
    "        ])\n",
    "\n",
    "    def reset(self):\n",
    "        self.architecture = []\n",
    "        self.done = False\n",
    "        return self._get_obs()\n",
    "\n",
    "    def _estimate_max_parameters(self):\n",
    "        input_dim = self.dataset[0].shape[1]  \n",
    "        max_units = 512\n",
    "        total_params = 0\n",
    "    \n",
    "        for _ in range(self.max_layers):\n",
    "            total_params += input_dim * max_units + max_units  \n",
    "            input_dim = max_units\n",
    "    \n",
    "        output_dim = self.dataset[1].shape[1]\n",
    "        total_params += max_units * output_dim + output_dim\n",
    "    \n",
    "        return total_params\n",
    "\n",
    "\n",
    "    def step(self, action_id):\n",
    "        decoded = decode_action(action_id)\n",
    "\n",
    "        dense_count = sum(1 for layer in self.architecture if layer[0] == 'dense')\n",
    "        if decoded[0] == 'stop' or (decoded[0] == 'dense' and dense_count >= self.max_layers):\n",
    "            self.done = True\n",
    "            reward, acc, complexity = self._evaluate_model()  \n",
    "\n",
    "            # Log architecture, reward, accuracy, and complexity\n",
    "            self.architecture_log.append({\n",
    "                'architecture': list(self.architecture),\n",
    "                'reward': reward,\n",
    "                'accuracy': acc,\n",
    "                'complexity': complexity\n",
    "            })\n",
    "    \n",
    "            print(f\"\\n🎯 Final Architecture: {self.architecture}\")\n",
    "            print(f\"🏆 Validation Accuracy (acc): {acc:.2f}%\")\n",
    "            print(f\"⚙️  Complexity (number of layers): {complexity}\")\n",
    "            print(f\"🏅 Reward (acc - penalty): {reward:.2f}%\\n\")\n",
    "\n",
    "            # Track best\n",
    "            if reward > self.best_reward:\n",
    "                self.best_reward = reward\n",
    "                self.best_architecture = list(self.architecture)\n",
    "                self.best_accuracy = acc  \n",
    "                self.best_complexity = complexity \n",
    "                print(f\"🌟 New Best Architecture Found with Reward: {reward:.2f}%\")\n",
    "        else:\n",
    "            self.architecture.append(decoded)\n",
    "            reward = 0\n",
    "\n",
    "        return self._get_obs(), reward, self.done, {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs = np.zeros(self.observation_space.shape[0])\n",
    "        for i, layer in enumerate(self.architecture):\n",
    "            base = i * 3\n",
    "            if layer[0] == 'dense':\n",
    "                obs[base] = layer[1] / 512  # normalized units\n",
    "                obs[base + 1] = activation_names.index(layer[2]) / (len(activation_names) - 1)\n",
    "                obs[base + 2] = 0\n",
    "            elif layer[0] == 'dropout':\n",
    "                obs[base] = layer[1]\n",
    "                obs[base + 1] = -1\n",
    "                obs[base + 2] = 1\n",
    "            elif layer[0] == 'batchnorm':\n",
    "                obs[base] = -1\n",
    "                obs[base + 1] = -1\n",
    "                obs[base + 2] = 2\n",
    "        obs[-len(self.dataset_features):] = self.dataset_features\n",
    "        return obs\n",
    "\n",
    "    def _evaluate_model(self):\n",
    "        X_train, y_train, X_val, y_val = self.dataset\n",
    "\n",
    "        model = nn.Sequential()\n",
    "        input_dim = X_train.shape[1]\n",
    "\n",
    "        for i, layer in enumerate(self.architecture):\n",
    "            if layer[0] == 'dense':\n",
    "                model.add_module(f\"fc{i}\", nn.Linear(input_dim, layer[1]))\n",
    "                model.add_module(f\"act{i}\", activation_map[layer[2]]())\n",
    "                input_dim = layer[1]\n",
    "            elif layer[0] == 'dropout':\n",
    "                model.add_module(f\"dropout{i}\", nn.Dropout(p=layer[1]))\n",
    "            elif layer[0] == 'batchnorm':\n",
    "                model.add_module(f\"bn{i}\", nn.BatchNorm1d(input_dim))\n",
    "\n",
    "        model.add_module(\"output\", nn.Linear(input_dim, y_train.shape[1]))\n",
    "\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01) # can make the RL choose optimizer and LR also here\n",
    "        train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "\n",
    "        model.train()\n",
    "        for _ in range(10):  \n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(xb)\n",
    "                loss = loss_fn(output, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = model(X_val)\n",
    "            acc = (preds.argmax(dim=1) == y_val.argmax(dim=1)).float().mean().item()\n",
    "\n",
    "        no_of_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        max_params = self._estimate_max_parameters()\n",
    "        alpha = 0.8 \n",
    "\n",
    "        reward = (alpha * acc * 100) - ((1 - alpha) * (no_of_params*100/max_params))\n",
    "\n",
    "        return reward, acc * 100, (no_of_params*100/max_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a11c62c-c969-4a13-aced-c2a9d0c38319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "import openml\n",
    "dataset = openml.datasets.get_dataset(23512)\n",
    "X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\n",
    "\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "X = df.drop(columns=[dataset.default_target_attribute])\n",
    "y = df[dataset.default_target_attribute]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)  \n",
    "\n",
    "y_oh = one_hot(torch.tensor(y_encoded)).float()\n",
    "\n",
    "X_train_np, X_val_np, y_train_tensor, y_val_tensor = train_test_split(\n",
    "    X_scaled, y_oh, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_np).float()\n",
    "X_val_tensor = torch.tensor(X_val_np).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96f71cae-ff9c-457f-80af-b132fc3c52a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17cb60a4-b94a-489b-8182-cf7e91314669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([88244, 28])\n",
      "torch.Size([88244, 2])\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tensor.shape)  # [n_train, n_features]\n",
    "print(y_train_tensor.shape)  # [n_train, 2]  (one-hot binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccf7a3f6-4ed4-4a12-a942-397f824dac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass into your NAS environment\n",
    "env = NASMLPEnv(dataset=dataset, max_layers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccac1e4-0745-4105-9621-3a85deac94d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./nas_logs/DQN_23\n",
      "\n",
      "🎯 Final Architecture: [('dense', 8, 'tanh'), ('dense', 8, 'sigmoid'), ('dense', 256, 'relu'), ('dense', 16, 'selu'), ('dense', 32, 'selu'), ('dense', 256, 'sigmoid'), ('dropout', 0.5), ('dense', 16, 'selu'), ('dense', 16, 'swish'), ('dense', 512, 'elu'), ('dense', 512, 'elu')]\n",
      "🏆 Validation Accuracy (acc): 68.39%\n",
      "⚙️  Complexity (number of layers): 12.290306070566245\n",
      "🏅 Reward (acc - penalty): 52.26%\n",
      "\n",
      "🌟 New Best Architecture Found with Reward: 52.26%\n",
      "\n",
      "🎯 Final Architecture: [('dense', 32, 'relu'), ('dense', 256, 'sigmoid'), ('dense', 32, 'swish'), ('dense', 64, 'sigmoid'), ('dropout', 0.2), ('dense', 128, 'elu'), ('dense', 16, 'tanh'), ('dense', 512, 'swish'), ('dense', 64, 'sigmoid'), ('dense', 512, 'relu'), ('dropout', 0.0), ('dense', 128, 'tanh')]\n",
      "🏆 Validation Accuracy (acc): 53.32%\n",
      "⚙️  Complexity (number of layers): 7.178568757253828\n",
      "🏅 Reward (acc - penalty): 41.22%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 32, 'tanh'), ('dense', 64, 'swish'), ('dense', 512, 'swish'), ('dense', 128, 'selu'), ('dense', 128, 'sigmoid'), ('dense', 256, 'swish'), ('dense', 256, 'tanh'), ('dense', 512, 'relu'), ('dropout', 0.0), ('dense', 16, 'relu'), ('dropout', 0.0), ('dense', 64, 'swish')]\n",
      "🏆 Validation Accuracy (acc): 53.32%\n",
      "⚙️  Complexity (number of layers): 15.056950690358512\n",
      "🏅 Reward (acc - penalty): 39.64%\n",
      "\n",
      "\n",
      "🎯 Final Architecture: [('dense', 64, 'elu'), ('dense', 64, 'elu'), ('dense', 128, 'relu'), ('dense', 64, 'relu'), ('dense', 128, 'sigmoid'), ('dropout', 0.0), ('dropout', 0.5), ('dense', 8, 'swish'), ('dense', 8, 'tanh'), ('dense', 32, 'relu'), ('dense', 512, 'swish'), ('dropout', 0.2), ('dense', 128, 'sigmoid')]\n",
      "🏆 Validation Accuracy (acc): 53.32%\n",
      "⚙️  Complexity (number of layers): 4.837510053458768\n",
      "🏅 Reward (acc - penalty): 41.69%\n",
      "\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13       |\n",
      "|    ep_rew_mean      | 43.7     |\n",
      "|    exploration_rate | 0.951    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 0        |\n",
      "|    time_elapsed     | 134      |\n",
      "|    total_timesteps  | 52       |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    gamma=0.99,                       \n",
    "    exploration_initial_eps=1.0,     \n",
    "    exploration_final_eps=0.05,      \n",
    "    exploration_fraction=0.1,        \n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./nas_logs/\"\n",
    ")\n",
    "\n",
    "#model = DQN(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./nas_logs/\")\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff3094d-6b96-4403-822f-8e004203ba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"nas_agent_checkpoint_data_specific_alpha_0.8_23512\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5413f2a9-b60e-4c3f-a84c-93c9da2ab94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5d8c53-11b2-4ea6-9725-e39ded168b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.best_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7ca670-01df-480d-9f01-e9537a5a202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n Best Reward: {env.best_reward:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77837302-2283-4651-bde0-b92782dc6f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n Best Accuracy: {env.best_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825a2636-af87-4c67-a710-7fa35a16ff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n Best Complexity: {env.best_complexity:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
